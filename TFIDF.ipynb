{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "TFIDF2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cyberbullying-Detection/Cyberbullying-Detection-on-Social-Media-using-Deep-Learning-and-Conventional-Machine-learning/blob/main/TFIDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "938787c5"
      },
      "source": [
        "import csv\n",
        "import random\n",
        "from collections import defaultdict, OrderedDict\n",
        "from operator import add\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#from tensorflow import keras\n",
        "import sklearn\n",
        "import sklearn.metrics as sm\n",
        "from sklearn import svm, tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "id": "938787c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daa49e5e"
      },
      "source": [
        "def shuffle_data(X, y):\n",
        "    combined = list(zip(X, y))\n",
        "    random.shuffle(combined)\n",
        "    X[:], y[:] = zip(*combined)\n",
        "    return X, y"
      ],
      "id": "daa49e5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5abc7285",
        "outputId": "39455e4d-d0a8-4705-eb10-35bbda70fc2b"
      },
      "source": [
        "data_file = pd.read_csv('Final Dataset/cleaned_tweets_16K.csv')\n",
        "data = data_file.drop(['contains_url','naughty_count','norm'],axis = 1)\n",
        "data.head(10)"
      ],
      "id": "5abc7285",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label_bullying</th>\n",
              "      <th>text_message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>another bloody instant restaurant week serious...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>this video of the peshmerga decimating isis i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>oh really no more instant restaurants thats sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>it hasnt been a good few weeks for isis a new ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>i don’t need femisnsn because men carry heavy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>19% is not the vast majority</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>and it is muslims who were the first crusader...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>capital hill is a great example of how seldom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>feminismisequalitywhen men are actually listen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>gather round kids its story time brought to yo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label_bullying                                       text_message\n",
              "0               0  another bloody instant restaurant week serious...\n",
              "1               0   this video of the peshmerga decimating isis i...\n",
              "2               0  oh really no more instant restaurants thats sh...\n",
              "3               0  it hasnt been a good few weeks for isis a new ...\n",
              "4               0  i don’t need femisnsn because men carry heavy ...\n",
              "5               0                       19% is not the vast majority\n",
              "6               1   and it is muslims who were the first crusader...\n",
              "7               1   capital hill is a great example of how seldom...\n",
              "8               1  feminismisequalitywhen men are actually listen...\n",
              "9               0  gather round kids its story time brought to yo..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d8fcd18"
      },
      "source": [
        "# Get TF-IDF Data"
      ],
      "id": "3d8fcd18"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7356d058"
      },
      "source": [
        "def get_term_freq_data(use_idf):\n",
        "    # Indicates if we are using TF or TF-IDF\n",
        "    corpus = []\n",
        "    y = []\n",
        "    USE_IDF = use_idf\n",
        "    dataset_filename='Final Dataset/cleaned_tweets_16K.csv'\n",
        "    print(\"Using IDF: \" + str(USE_IDF))\n",
        "\n",
        "    # GET THE DATA\n",
        "    #corpus, y = get_data(dataset_filename)\n",
        "    with open(dataset_filename,newline='',encoding=\"utf8\") as csvfile:\n",
        "        csv_reader = csv.reader(csvfile, delimiter=',')\n",
        "        line_count = 0\n",
        "        \n",
        "        for row in csv_reader:\n",
        "            if line_count == 0:\n",
        "                print(','.join(row))\n",
        "            else:\n",
        "                corpus.append(row[1])\n",
        "                y.append(int(row[0]))\n",
        "            line_count += 1\n",
        "            \n",
        "    print(\"vectorising...\")\n",
        "    vec = TfidfVectorizer(min_df=0.001, max_df=1.0)\n",
        "    \n",
        "    # shuffle the data so that it is randomised\n",
        "    X, Y = shuffle_data(corpus, y)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.50, random_state=14)  # 5 is best\n",
        "    corpus_fit_transform = vec.fit_transform(corpus)\n",
        "\n",
        "    newVec = TfidfVectorizer(vocabulary=vec.vocabulary_, use_idf=USE_IDF)\n",
        "    X_train = newVec.fit_transform(X_train).toarray()\n",
        "    X_test = newVec.fit_transform(X_test).toarray()\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "    print()\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "id": "7356d058",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "546baf5b"
      },
      "source": [
        "# Logistic Regression"
      ],
      "id": "546baf5b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30b1751f",
        "outputId": "a01e45df-0cd1-4094-da34-36c438f531e1"
      },
      "source": [
        "#Logistic Regression\n",
        "X_train_logistic, X_test_logistic, y_train_logistic, y_test_logistic = get_term_freq_data(use_idf=True)\n",
        "#repeat positives\n",
        "# grams_X_train_logistic, grams_y_train_logistic = repeat_positives(X_train_logistic, y_train_logistic, repeats=2)\n",
        "\n",
        "grid_searching = False\n",
        "clf = sklearn.linear_model.LogisticRegression(penalty=\"l2\", max_iter=100, solver=\"liblinear\")\n",
        "clf = clf.fit(X_train_logistic, y_train_logistic)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_logistic = clf.predict(X_test_logistic)\n",
        "print(y_pred_logistic)\n"
      ],
      "id": "30b1751f",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using IDF: True\n",
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "vectorising...\n",
            "(7883, 1445)\n",
            "(7883, 1445)\n",
            "\n",
            "\n",
            "evaluating\n",
            "[0 0 0 ... 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be3bcad8",
        "outputId": "bddc55bb-967b-40d4-b882-e87b53eee8c5"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_logistic,y_pred_logistic))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_logistic,y_pred_logistic), 4))"
      ],
      "id": "be3bcad8",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[5083  306]\n",
            " [1174 1320]]\n",
            "accuracy: 0.8123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13ce5a22"
      },
      "source": [
        "# Random Forest"
      ],
      "id": "13ce5a22"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d33ef6c3",
        "outputId": "d259350f-bd4d-41f0-a4f0-2c8a6ed314a7"
      },
      "source": [
        "#Random Forest\n",
        "X_train_random, X_test_random, y_train_random, y_test_random = get_term_freq_data(use_idf=True)\n",
        "# repeat positives\n",
        "# grams_X_train_random, grams_y_train_random = repeat_positives(X_train_random, y_train_random, repeats=2)\n",
        "grid_searching = False\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=4)\n",
        "clf = clf.fit(X_train_random, y_train_random)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_random = clf.predict(X_test_random)\n",
        "print(y_pred_random)"
      ],
      "id": "d33ef6c3",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using IDF: True\n",
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "vectorising...\n",
            "(7883, 1445)\n",
            "(7883, 1445)\n",
            "\n",
            "\n",
            "evaluating\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee015b80",
        "outputId": "2aad54c0-519b-423e-d35f-dbc12aafd55a"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_random,y_pred_random))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_random,y_pred_random), 4))"
      ],
      "id": "ee015b80",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[5345    0]\n",
            " [2508   30]]\n",
            "accuracy: 0.6818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd7ecef5"
      },
      "source": [
        "# Bernoulli Naive Bayes"
      ],
      "id": "bd7ecef5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "422a8fe5",
        "outputId": "71838751-4b5b-4122-9290-ed6ccfb73088"
      },
      "source": [
        "X_train_bayes, X_test_bayes, y_train_bayes, y_test_bayes = get_term_freq_data(use_idf=True)\n",
        "#repeat positives\n",
        "# grams_X_train_bayes, grams_y_train_bayes = repeat_positives(X_train_bayes, y_train_bayes, repeats=2)\n",
        "grid_searching = False\n",
        "clf = BernoulliNB()\n",
        "clf = clf.fit(X_train_bayes, y_train_bayes)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_bayes = clf.predict(X_test_bayes)\n",
        "print(y_pred_bayes)"
      ],
      "id": "422a8fe5",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using IDF: True\n",
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "vectorising...\n",
            "(7883, 1445)\n",
            "(7883, 1445)\n",
            "\n",
            "\n",
            "evaluating\n",
            "[0 1 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "187a73da",
        "outputId": "bbc21663-b46f-4089-ae35-e035d9da00d6"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_bayes,y_pred_bayes))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_bayes,y_pred_bayes), 4))"
      ],
      "id": "187a73da",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[4733  637]\n",
            " [ 948 1565]]\n",
            "accuracy: 0.7989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6282a625"
      },
      "source": [
        "# KNN"
      ],
      "id": "6282a625"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4330e2d6",
        "outputId": "6a1b370e-5204-4634-880e-2cb8973cedfe"
      },
      "source": [
        "X_train_knn, X_test_knn, y_train_knn, y_test_knn = get_term_freq_data(use_idf=True)\n",
        "# repeat positives\n",
        "# grams_X_train_knn, grams_y_train_knn = repeat_positives(X_train_knn, y_train_knn, repeats=2)\n",
        "grid_searching = False\n",
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "clf = clf.fit(X_train_knn, y_train_knn)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_knn = clf.predict(X_test_knn)\n",
        "print(y_pred_knn)"
      ],
      "id": "4330e2d6",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using IDF: True\n",
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "vectorising...\n",
            "(7883, 1445)\n",
            "(7883, 1445)\n",
            "\n",
            "\n",
            "evaluating\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c3114cd",
        "outputId": "47d13714-7fce-4ae0-8371-7e931d7d2f74"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_knn,y_pred_knn))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_knn,y_pred_knn), 4))"
      ],
      "id": "7c3114cd",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[5284   87]\n",
            " [2221  291]]\n",
            "accuracy: 0.7072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a07b611"
      },
      "source": [
        "# AdaBoost Classifier"
      ],
      "id": "1a07b611"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e74d4c6",
        "outputId": "8af8d289-00f5-4742-abb6-4e90162d7b18"
      },
      "source": [
        "X_train_adaboost, X_test_adaboost, y_train_adaboost, y_test_adaboost = get_term_freq_data(use_idf=True)\n",
        "#repeat positives\n",
        "# grams_X_train_adaboost, grams_y_train_adaboost = repeat_positives(X_train_adaboost, y_train_adaboost, repeats=2)\n",
        "grid_searching = False\n",
        "clf = AdaBoostClassifier()\n",
        "clf = clf.fit(X_train_adaboost, y_train_adaboost)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_adaboost = clf.predict(X_test_adaboost)\n",
        "print(y_pred_adaboost)"
      ],
      "id": "3e74d4c6",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using IDF: True\n",
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "vectorising...\n",
            "(7883, 1445)\n",
            "(7883, 1445)\n",
            "\n",
            "\n",
            "evaluating\n",
            "[0 0 0 ... 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "798346e6",
        "outputId": "fbfdd5b3-989c-4e32-eb2d-b7224fd858d3"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_adaboost,y_pred_adaboost))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_adaboost,y_pred_adaboost), 4))"
      ],
      "id": "798346e6",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[5001  355]\n",
            " [1140 1387]]\n",
            "accuracy: 0.8104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ce14cf5"
      },
      "source": [
        "# SVM"
      ],
      "id": "9ce14cf5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dec6851",
        "outputId": "e29972f3-55da-4c73-f1cd-18ff38e60076"
      },
      "source": [
        "X_train_svm, X_test_svm, y_train_svm, y_test_svm = get_term_freq_data(use_idf=True)\n",
        "#repeat positives\n",
        "#X_train_svm, y_train_svm = repeat_positives(X_train_svm, y_train_svm, repeats=2)\n",
        "grid_searching = False\n",
        "clf = svm.SVC(C=10, kernel=\"rbf\", gamma=0.001)\n",
        "clf = clf.fit(X_train_svm, y_train_svm)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_svm = clf.predict(X_test_svm)\n",
        "print(y_pred_svm)"
      ],
      "id": "8dec6851",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using IDF: True\n",
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "vectorising...\n",
            "(7883, 1445)\n",
            "(7883, 1445)\n",
            "\n",
            "\n",
            "evaluating\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4d9562c",
        "outputId": "330e4430-78ff-44ee-e776-ef29208c6566"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_svm,y_pred_svm))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_svm,y_pred_svm), 4))"
      ],
      "id": "d4d9562c",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[5324    4]\n",
            " [2432  123]]\n",
            "accuracy: 0.691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "906bf9ea"
      },
      "source": [
        "# DECISION TREE"
      ],
      "id": "906bf9ea"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d02f62d",
        "outputId": "6733bc8b-1f84-4e3e-c748-11aa8bce42e2"
      },
      "source": [
        "X_train_tree, X_test_tree, y_train_tree, y_test_tree = get_term_freq_data(use_idf=True)\n",
        "#repeat positives\n",
        "#grams_X_train_tree, grams_y_train_tree = repeat_positives(X_train_tree, y_train_tree, repeats=2)\n",
        "grid_searching = False\n",
        "clf = clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train_tree, y_train_tree)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_tree = clf.predict(X_test_tree)\n",
        "print(y_pred_tree)"
      ],
      "id": "2d02f62d",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using IDF: True\n",
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "vectorising...\n",
            "(7883, 1445)\n",
            "(7883, 1445)\n",
            "\n",
            "\n",
            "evaluating\n",
            "[0 0 1 ... 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9229d119",
        "outputId": "9cabf1cb-5993-49bd-c8e2-1f5e8a108859"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_tree,y_pred_tree))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_tree,y_pred_tree), 4))"
      ],
      "id": "9229d119",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[4428  936]\n",
            " [ 979 1540]]\n",
            "accuracy: 0.7571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5430562",
        "outputId": "372b404e-1071-4bf4-b166-fabdf77d09ba"
      },
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "#Logistic\n",
        "L_Recall = round(sm.recall_score(y_test_logistic, y_pred_logistic),4)\n",
        "L_Precision = round(sm.precision_score(y_test_logistic, y_pred_logistic),4)\n",
        "L_F1_score = round(sm.f1_score(y_test_logistic, y_pred_logistic), 4)\n",
        "print(\"Logistic recall:\",L_Recall )\n",
        "print(\"Logistic precision:\",L_Precision)\n",
        "print(\"Logistic f1 score:\",L_F1_score )\n",
        "\n",
        "#Random\n",
        "R_Recall = round(sm.recall_score(y_test_random, y_pred_random), 4)\n",
        "R_Precision = round(sm.precision_score(y_test_random, y_pred_random), 4)\n",
        "R_F1_score = round(sm.f1_score(y_test_random, y_pred_random), 4)\n",
        "print(\"Random recall:\",R_Recall )\n",
        "print(\"Random precision:\", R_Precision)\n",
        "print(\"Random f1 score:\",R_F1_score )\n",
        "\n",
        "#Naive\n",
        "N_Recall = round(sm.recall_score(y_test_bayes, y_pred_bayes), 4)\n",
        "N_Precision =round(sm.precision_score(y_test_bayes, y_pred_bayes), 4)\n",
        "N_F1_score = round(sm.f1_score(y_test_bayes, y_pred_bayes), 4)\n",
        "print(\"Naive recall:\", N_Recall)\n",
        "print(\"Naive precision:\",N_Precision )\n",
        "print(\"Naive f1 score:\", N_F1_score )\n",
        "\n",
        "#KNN\n",
        "KNN_Recall = round(sm.recall_score(y_test_knn, y_pred_knn), 4)\n",
        "KNN_Precision =round(sm.precision_score(y_test_knn, y_pred_knn), 4)\n",
        "KNN_F1_score = round(sm.f1_score(y_test_knn, y_pred_knn), 4)\n",
        "print(\"KNN recall:\", KNN_Recall )\n",
        "print(\"KNN precision:\", KNN_Precision)\n",
        "print(\"KNN f1 score:\", KNN_F1_score )\n",
        "\n",
        "#Adaboost\n",
        "Adaboost_Recall = round(sm.recall_score(y_test_adaboost, y_pred_adaboost), 4)\n",
        "Adaboost_Precision = round(sm.precision_score(y_test_adaboost, y_pred_adaboost), 4)\n",
        "Adaboost_F1_score = round(sm.f1_score(y_test_adaboost, y_pred_adaboost), 4)\n",
        "print(\"KNN recall:\",Adaboost_Recall )\n",
        "print(\"KNN precision:\",Adaboost_Precision )\n",
        "print(\"KNN f1 score:\", Adaboost_F1_score)\n",
        "\n",
        "#SVM\n",
        "svm_Recall = round(sm.recall_score(y_test_svm, y_pred_svm), 4)\n",
        "svm_Precision = round(sm.precision_score(y_test_svm, y_pred_svm), 4)\n",
        "svm_F1_score = round(sm.f1_score(y_test_svm, y_pred_svm), 4)\n",
        "print(\"SVM recall:\",svm_Recall )\n",
        "print(\"SVM precision:\", svm_Precision )\n",
        "print(\"SVM f1 score:\", svm_F1_score )\n",
        "\n",
        "#Decision\n",
        "decision_Recall = round(sm.recall_score(y_test_tree, y_pred_tree), 4)\n",
        "decision_Precision = round(sm.precision_score(y_test_tree, y_pred_tree), 4)\n",
        "decision_F1_score = round(sm.f1_score(y_test_tree, y_pred_tree), 4)\n",
        "print(\"Decision recall:\", decision_Recall)\n",
        "print(\"Decision precision:\",decision_Precision )\n",
        "print(\"Decision f1 score:\", decision_F1_score )\n",
        "\n",
        "\n",
        "# final_data = [L_Recall,L_Precision, L_F1 score, R_Recall, \n",
        "#               R_Precision,R_F1 score,N_Recall,N_Precision, N_F1 score,\n",
        "#               Adaboost_Recall,Adaboost_Precision,Adaboost_F1 score,svm_Recall,\n",
        "#              svm_Precision,svm_F1 score,decision_Recall,decision_Precision,decision_F1 score]\n",
        "\n",
        "\n",
        "with open('TFIDF_Final_data5.csv','w') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Recall','Precision','F1-score'])\n",
        "    writer.writerow([L_Recall,L_Precision,L_F1_score])\n",
        "    writer.writerow([R_Recall, R_Precision,R_F1_score])\n",
        "    writer.writerow([N_Recall,N_Precision, N_F1_score])\n",
        "    writer.writerow([KNN_Recall,KNN_Precision,KNN_F1_score])\n",
        "    writer.writerow([Adaboost_Recall,Adaboost_Precision,Adaboost_F1_score])\n",
        "    writer.writerow([svm_Recall,svm_Precision,svm_F1_score])\n",
        "    writer.writerow([decision_Recall,decision_Precision,decision_F1_score])\n",
        "    \n",
        "    "
      ],
      "id": "b5430562",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic recall: 0.5293\n",
            "Logistic precision: 0.8118\n",
            "Logistic f1 score: 0.6408\n",
            "Random recall: 0.0118\n",
            "Random precision: 1.0\n",
            "Random f1 score: 0.0234\n",
            "Naive recall: 0.6228\n",
            "Naive precision: 0.7107\n",
            "Naive f1 score: 0.6638\n",
            "KNN recall: 0.1158\n",
            "KNN precision: 0.7698\n",
            "KNN f1 score: 0.2014\n",
            "KNN recall: 0.5489\n",
            "KNN precision: 0.7962\n",
            "KNN f1 score: 0.6498\n",
            "SVM recall: 0.0481\n",
            "SVM precision: 0.9685\n",
            "SVM f1 score: 0.0917\n",
            "Decision recall: 0.6114\n",
            "Decision precision: 0.622\n",
            "Decision f1 score: 0.6166\n"
          ]
        }
      ]
    }
  ]
}