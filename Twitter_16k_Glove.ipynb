{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Twitter_16k_Glove.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cyberbullying-Detection/Cyberbullying-Detection-on-Social-Media-using-Deep-Learning-and-Conventional-Machine-learning/blob/main/Twitter_16k_Glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4bxHq2PcwwP"
      },
      "source": [
        "import csv\n",
        "import random\n",
        "from collections import defaultdict, OrderedDict\n",
        "from operator import add\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import sklearn\n",
        "import sklearn.metrics as sm\n",
        "from sklearn import svm, tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "id": "M4bxHq2PcwwP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-T8ftRHcwwW"
      },
      "source": [
        "def shuffle_data(X, y):\n",
        "    combined = list(zip(X, y))\n",
        "    random.shuffle(combined)\n",
        "    X[:], y[:] = zip(*combined)\n",
        "    return X, y"
      ],
      "id": "T-T8ftRHcwwW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEdhPfivcwwY"
      },
      "source": [
        "# Get Glove Data"
      ],
      "id": "gEdhPfivcwwY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PhvVgTZcwwa"
      },
      "source": [
        "def get_glove_data():\n",
        "    comments = []\n",
        "    y = []\n",
        "    dataset_filename='Final Dataset/cleaned_tweets_16K.csv'\n",
        "    \n",
        "    with open(dataset_filename,newline='',encoding=\"utf8\") as csvfile:\n",
        "        csv_reader = csv.reader(csvfile, delimiter=',')\n",
        "        line_count = 0\n",
        "        \n",
        "        for row in csv_reader:\n",
        "            if line_count == 0:\n",
        "                print(','.join(row))\n",
        "            else:\n",
        "                comments.append(row[1])\n",
        "                y.append(int(row[0]))\n",
        "            line_count += 1\n",
        "    \n",
        "    num_comments = len(comments)\n",
        "    print(\"splitting data........\")\n",
        "    word_arrays = []\n",
        "    for s in comments:\n",
        "        word_arrays.append(s.split(' '))\n",
        "        \n",
        "    print(\"Getting GLOVE embeddings size 50..\")\n",
        "    file = open('glove.6B/glove.6B.50d.txt',errors='ignore').readlines()\n",
        "    gloveDict = {}\n",
        "    for line in file:\n",
        "        info = line.split(' ')\n",
        "        key = info[0]\n",
        "        vec = []\n",
        "        for elem in info[1:]:\n",
        "            vec.append(elem.rstrip())\n",
        "        gloveDict[key] = vec\n",
        "    print(len(gloveDict),\"words in the GLOVE dictionary\\n\")\n",
        "    \n",
        "    #VECTORISE WORDS\n",
        "    print(\"converting comments to lists of vectors........\")\n",
        "    word_vectors = []\n",
        "    for sentence in word_arrays:\n",
        "        temp = []\n",
        "        for word in sentence:\n",
        "            if word in gloveDict:\n",
        "                temp.append(gloveDict[word])\n",
        "        word_vectors.append(temp)\n",
        "        \n",
        "    MAX_LEN = 32\n",
        "    \n",
        "    print(\"padding vectors to maxlen = \",MAX_LEN,\".....\")\n",
        "    padded_word_vecs = np.array(pad_sequences(word_vectors, padding='pre', maxlen=MAX_LEN, dtype='float32'))\n",
        "    padded_word_vecs = padded_word_vecs.reshape((num_comments, -1))\n",
        "    \n",
        "    print(\"DONE PRE-PROCESSING\\n\")\n",
        "    \n",
        "    #CLASSIFYING\n",
        "    print(\"splitting\")\n",
        "    X_train,X_test,y_train,y_test = train_test_split(padded_word_vecs,y,test_size=0.20)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "id": "5PhvVgTZcwwa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b6SixmVcwwc"
      },
      "source": [
        "# Logistic Regression"
      ],
      "id": "0b6SixmVcwwc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnYBNqUqcwwd",
        "outputId": "5e8934de-4de7-4b91-ba77-e81992b26f66"
      },
      "source": [
        "#Logistic Regression\n",
        "X_train_logistic, X_test_logistic, y_train_logistic, y_test_logistic = get_glove_data()\n",
        "\n",
        "grid_searching = False\n",
        "clf = sklearn.linear_model.LogisticRegression(penalty=\"l2\", max_iter=100, solver=\"liblinear\")\n",
        "clf = clf.fit(X_train_logistic, y_train_logistic)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_logistic = clf.predict(X_test_logistic)\n",
        "print(y_pred_logistic)"
      ],
      "id": "qnYBNqUqcwwd",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "splitting data........\n",
            "Getting GLOVE embeddings size 50..\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-4-647eff0f12df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Logistic Regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train_logistic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_logistic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_logistic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_logistic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_glove_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#repeat positives\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# grams_X_train_logistic, grams_y_train_logistic = repeat_positives(X_train_logistic, y_train_logistic, repeats=2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-3-3cd2f198a5da>\u001b[0m in \u001b[0;36mget_glove_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mgloveDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgloveDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"words in the GLOVE dictionary\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knMaxjJycwwf",
        "outputId": "1365f788-5b41-41a9-c20f-d1538af12732"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_logistic,y_pred_logistic))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_logistic,y_pred_logistic), 4))"
      ],
      "id": "knMaxjJycwwf",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[1855  276]\n",
            " [ 491  532]]\n",
            "accuracy: 0.7568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG8u4KHbcwwf",
        "outputId": "f0a639e2-c950-4f17-aaff-5c48a3e6607e"
      },
      "source": [
        "print(\"recall:\", round(sm.recall_score(y_test_random, y_pred_random), 4))\n",
        "print(\"precision:\", round(sm.precision_score(y_test_random, y_pred_random), 4))\n",
        "print(\"f1 score:\", round(sm.f1_score(y_test_random, y_pred_random), 4))"
      ],
      "id": "XG8u4KHbcwwf",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall: 0.0248\n",
            "precision: 0.8\n",
            "f1 score: 0.0481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp3VdeUecwwg"
      },
      "source": [
        "# Random Forest"
      ],
      "id": "qp3VdeUecwwg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeMtO8pLcwwg",
        "outputId": "deeed5ae-a198-413e-9be8-536b3e41584c"
      },
      "source": [
        "#Logistic Regression\n",
        "X_train_random, X_test_random, y_train_random, y_test_random = get_glove_data()\n",
        "grid_searching = False\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=4)\n",
        "clf = clf.fit(X_train_random, y_train_random)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_random = clf.predict(X_test_random)\n",
        "print(y_pred_random)"
      ],
      "id": "AeMtO8pLcwwg",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "splitting data........\n",
            "Getting GLOVE embeddings size 50..\n",
            "399997 words in the GLOVE dictionary\n",
            "\n",
            "converting comments to lists of vectors........\n",
            "padding vectors to maxlen =  32 .....\n",
            "DONE PRE-PROCESSING\n",
            "\n",
            "splitting\n",
            "\n",
            "evaluating\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV546w1pcwwh",
        "outputId": "5701a02b-d355-45bb-bc41-f47e879340c7"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_random,y_pred_random))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_random,y_pred_random), 4))"
      ],
      "id": "EV546w1pcwwh",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[2180    6]\n",
            " [ 944   24]]\n",
            "accuracy: 0.6988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMLo0g5Gcwwh",
        "outputId": "eb7e4654-e11b-4c91-9c73-634b39597d20"
      },
      "source": [
        "print(\"recall:\", round(sm.recall_score(y_test_random, y_pred_random), 4))\n",
        "print(\"precision:\", round(sm.precision_score(y_test_random, y_pred_random), 4))\n",
        "print(\"f1 score:\", round(sm.f1_score(y_test_random, y_pred_random), 4))"
      ],
      "id": "dMLo0g5Gcwwh",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall: 0.0248\n",
            "precision: 0.8\n",
            "f1 score: 0.0481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2vQCAAkcwwi"
      },
      "source": [
        "# Bernoulli Naive Bayes"
      ],
      "id": "U2vQCAAkcwwi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKTHls0Acwwi",
        "outputId": "7e05ae64-b55a-461e-a2cf-c86b7c0851f5"
      },
      "source": [
        "X_train_bayes, X_test_bayes, y_train_bayes, y_test_bayes = get_glove_data()\n",
        "grid_searching = False\n",
        "clf = BernoulliNB()\n",
        "clf = clf.fit(X_train_bayes, y_train_bayes)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_bayes = clf.predict(X_test_bayes)\n",
        "print(y_pred_bayes)\n"
      ],
      "id": "HKTHls0Acwwi",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "splitting data........\n",
            "Getting GLOVE embeddings size 50..\n",
            "399997 words in the GLOVE dictionary\n",
            "\n",
            "converting comments to lists of vectors........\n",
            "padding vectors to maxlen =  32 .....\n",
            "DONE PRE-PROCESSING\n",
            "\n",
            "splitting\n",
            "\n",
            "evaluating\n",
            "[0 0 0 ... 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWsVN7u_cwwj",
        "outputId": "86f96bf5-c6ad-4ecc-f914-24e20d768219"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_bayes,y_pred_bayes))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_bayes,y_pred_bayes), 4))"
      ],
      "id": "wWsVN7u_cwwj",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[1256  917]\n",
            " [ 387  594]]\n",
            "accuracy: 0.5866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jH92lJIcwwj",
        "outputId": "000bb4f1-0169-4e70-e1f8-5b3b4f32b903"
      },
      "source": [
        "print(\"recall:\", round(sm.recall_score(y_test_bayes, y_pred_bayes), 4))\n",
        "print(\"precision:\", round(sm.precision_score(y_test_bayes, y_pred_bayes), 4))\n",
        "print(\"f1 score:\", round(sm.f1_score(y_test_bayes, y_pred_bayes), 4))"
      ],
      "id": "9jH92lJIcwwj",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall: 0.6055\n",
            "precision: 0.3931\n",
            "f1 score: 0.4767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsrpXHdzcwwk"
      },
      "source": [
        "# KNN"
      ],
      "id": "ZsrpXHdzcwwk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHNCrGQmcwwl",
        "outputId": "5aff5e19-7a64-46d5-a62f-faa8b18ce895"
      },
      "source": [
        "X_train_knn, X_test_knn, y_train_knn, y_test_knn = get_glove_data()\n",
        "grid_searching = False\n",
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "clf = clf.fit(X_train_knn, y_train_knn)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_knn = clf.predict(X_test_knn)\n",
        "print(y_pred_knn)"
      ],
      "id": "HHNCrGQmcwwl",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "splitting data........\n",
            "Getting GLOVE embeddings size 50..\n",
            "399997 words in the GLOVE dictionary\n",
            "\n",
            "converting comments to lists of vectors........\n",
            "padding vectors to maxlen =  32 .....\n",
            "DONE PRE-PROCESSING\n",
            "\n",
            "splitting\n",
            "\n",
            "evaluating\n",
            "[0 0 0 ... 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlFReDH1cwwl",
        "outputId": "7af2452e-c3d2-41e3-ac72-c2aa73926d32"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_knn,y_pred_knn))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_knn,y_pred_knn), 4))"
      ],
      "id": "rlFReDH1cwwl",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[1933  222]\n",
            " [ 649  350]]\n",
            "accuracy: 0.7238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhfZL47Ecwwm",
        "outputId": "f547df09-405a-413d-f8bf-500d0499379e"
      },
      "source": [
        "print(\"recall:\", round(sm.recall_score(y_test_knn, y_pred_knn), 4))\n",
        "print(\"precision:\", round(sm.precision_score(y_test_knn, y_pred_knn), 4))\n",
        "print(\"f1 score:\", round(sm.f1_score(y_test_knn, y_pred_knn), 4))"
      ],
      "id": "XhfZL47Ecwwm",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall: 0.3504\n",
            "precision: 0.6119\n",
            "f1 score: 0.4456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3SrYpDucwwm"
      },
      "source": [
        "# Adaboost Classifier"
      ],
      "id": "j3SrYpDucwwm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uZSrfZPcwwm",
        "outputId": "b1dde37e-0343-4d5a-e9e4-46259d785441"
      },
      "source": [
        "X_train_adaboost, X_test_adaboost, y_train_adaboost, y_test_adaboost = get_glove_data()\n",
        "grid_searching = False\n",
        "clf = AdaBoostClassifier()\n",
        "clf = clf.fit(X_train_adaboost, y_train_adaboost)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_adaboost = clf.predict(X_test_adaboost)\n",
        "print(y_pred_adaboost)"
      ],
      "id": "6uZSrfZPcwwm",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "splitting data........\n",
            "Getting GLOVE embeddings size 50..\n",
            "399997 words in the GLOVE dictionary\n",
            "\n",
            "converting comments to lists of vectors........\n",
            "padding vectors to maxlen =  32 .....\n",
            "DONE PRE-PROCESSING\n",
            "\n",
            "splitting\n",
            "\n",
            "evaluating\n",
            "[0 0 0 ... 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_7Y0nKtcwwn",
        "outputId": "418efaba-97ff-411c-eb13-1dd4bdc1794b"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_adaboost,y_pred_adaboost))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_adaboost,y_pred_adaboost), 4))"
      ],
      "id": "s_7Y0nKtcwwn",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[1919  223]\n",
            " [ 597  415]]\n",
            "accuracy: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfwppHe-cwwn",
        "outputId": "edc28522-0a13-440f-b65b-560d39e860eb"
      },
      "source": [
        "print(\"recall:\", round(sm.recall_score(y_test_adaboost, y_pred_adaboost), 4))\n",
        "print(\"precision:\", round(sm.precision_score(y_test_adaboost, y_pred_adaboost), 4))\n",
        "print(\"f1 score:\", round(sm.f1_score(y_test_adaboost, y_pred_adaboost), 4))"
      ],
      "id": "GfwppHe-cwwn",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall: 0.4101\n",
            "precision: 0.6505\n",
            "f1 score: 0.503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux1kx8Mlcwwo"
      },
      "source": [
        "# SVM"
      ],
      "id": "ux1kx8Mlcwwo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waEmUFupcwwp",
        "outputId": "eae9bdbb-24f3-44c4-e508-3f54055da392"
      },
      "source": [
        "X_train_svm, X_test_svm, y_train_svm, y_test_svm = get_glove_data()\n",
        "grid_searching = False\n",
        "clf = svm.SVC(C=10, kernel=\"rbf\", gamma=0.001)\n",
        "clf = clf.fit(X_train_svm, y_train_svm)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_svm = clf.predict(X_test_svm)\n",
        "print(y_pred_svm)"
      ],
      "id": "waEmUFupcwwp",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "splitting data........\n",
            "Getting GLOVE embeddings size 50..\n",
            "399997 words in the GLOVE dictionary\n",
            "\n",
            "converting comments to lists of vectors........\n",
            "padding vectors to maxlen =  32 .....\n",
            "DONE PRE-PROCESSING\n",
            "\n",
            "splitting\n",
            "\n",
            "evaluating\n",
            "[0 0 0 ... 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JumPpbhzcwwp",
        "outputId": "e63f7d19-247b-4024-ae7d-e70f6edca70a"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_svm,y_pred_svm))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_svm,y_pred_svm), 4))"
      ],
      "id": "JumPpbhzcwwp",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[1958  200]\n",
            " [ 507  489]]\n",
            "accuracy: 0.7758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmaqQmL4cwwp",
        "outputId": "89b81219-452c-456f-83d7-e284adb5b3c0"
      },
      "source": [
        "print(\"recall:\", round(sm.recall_score(y_test_svm, y_pred_svm), 4))\n",
        "print(\"precision:\", round(sm.precision_score(y_test_svm, y_pred_svm), 4))\n",
        "print(\"f1 score:\", round(sm.f1_score(y_test_svm, y_pred_svm), 4))"
      ],
      "id": "cmaqQmL4cwwp",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall: 0.491\n",
            "precision: 0.7097\n",
            "f1 score: 0.5804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1h_3w1ncwwq"
      },
      "source": [
        "# Decision Tree"
      ],
      "id": "q1h_3w1ncwwq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtEy0S3Acwwq",
        "outputId": "0e1c7d86-c98e-4ecc-a038-02a527f3da22"
      },
      "source": [
        "X_train_tree, X_test_tree, y_train_tree, y_test_tree = get_glove_data()\n",
        "grid_searching = False\n",
        "clf = clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train_tree, y_train_tree)\n",
        "\n",
        "#PREDICT\n",
        "print(\"\\nevaluating\")\n",
        "y_pred_tree = clf.predict(X_test_tree)\n",
        "print(y_pred_tree)"
      ],
      "id": "CtEy0S3Acwwq",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label_bullying,text_message,contains_url,naughty_count,norm\n",
            "splitting data........\n",
            "Getting GLOVE embeddings size 50..\n",
            "399997 words in the GLOVE dictionary\n",
            "\n",
            "converting comments to lists of vectors........\n",
            "padding vectors to maxlen =  32 .....\n",
            "DONE PRE-PROCESSING\n",
            "\n",
            "splitting\n",
            "\n",
            "evaluating\n",
            "[0 0 0 ... 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqvDYAlqcwwr",
        "outputId": "05b61958-09df-4402-d4e5-3f14a5498aeb"
      },
      "source": [
        "# EVALUATE\n",
        "print(\"confusion matrix:\\n\", sm.confusion_matrix(y_test_tree,y_pred_tree))\n",
        "print(\"accuracy:\", round(sm.accuracy_score(y_test_tree,y_pred_tree), 4))"
      ],
      "id": "iqvDYAlqcwwr",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix:\n",
            " [[1597  546]\n",
            " [ 525  486]]\n",
            "accuracy: 0.6604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB4fUbN2cwwr",
        "outputId": "a8298be7-58a5-4738-a597-89b84fbcc11d"
      },
      "source": [
        "print(\"recall:\", round(sm.recall_score(y_test_tree, y_pred_tree), 4))\n",
        "print(\"precision:\", round(sm.precision_score(y_test_tree, y_pred_tree), 4))\n",
        "print(\"f1 score:\", round(sm.f1_score(y_test_tree, y_pred_tree), 4))"
      ],
      "id": "eB4fUbN2cwwr",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall: 0.4807\n",
            "precision: 0.4709\n",
            "f1 score: 0.4758\n"
          ]
        }
      ]
    }
  ]
}